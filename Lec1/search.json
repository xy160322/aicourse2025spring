[
  {
    "objectID": "Lec1.html#例北京某小区房价数据",
    "href": "Lec1.html#例北京某小区房价数据",
    "title": "0x01 线性回归",
    "section": "例：北京某小区房价数据",
    "text": "例：北京某小区房价数据\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\narea\nprice\n\n\n\n\n0\n62.9\n680\n\n\n1\n44.3\n485\n\n\n2\n57.4\n555\n\n\n3\n62.4\n620\n\n\n4\n57.4\n585\n\n\n5\n…\n…\n\n\n\n\n\n\n\n行、列，\n特征（属性）\n标签\n数据下载\n\n\n\nSpeaker notes go here."
  },
  {
    "objectID": "Lec1.html#问题与假设",
    "href": "Lec1.html#问题与假设",
    "title": "0x01 线性回归",
    "section": "问题与假设",
    "text": "问题与假设\n\n问题：给定一组住房成交数据，每条数据有一个属性（面积 \\(area\\) ），一个标签（价格 \\(price\\) ），建立模型，能够实现给定一个新挂牌的房子的面积，预测它的成交价\n假设：所有\\((area,price)\\)点在同一条直线附近"
  },
  {
    "objectID": "Lec1.html#建模",
    "href": "Lec1.html#建模",
    "title": "0x01 线性回归",
    "section": "建模",
    "text": "建模\n\n训练集 \\(T\\)：采集到的样本数据\n\n一个训练数据记为 \\((x,y)\\)，第 \\(i\\) 个训练数据记为 \\((x_i,y_i)\\)\n训练集大小：\\(m\\)\n\n\\(T=\\{(x_1,y_1),(x_2,y_2),...,(x_m,y_m)\\}=\\{(x_i,y_i)\\}\\)\n\n决策函数假设：\\(f(x)=kx+b\\)，属性 \\(x\\) 相应的预测结果 \\(f(x)\\)\n损失函数: \\(L(k,b;T)\\)，在已知训练集 \\(T\\) 的情况下关于决策函数参数 \\(k,b\\) 的函数，用于量化参数的效果，即在训练集上预测值与真实值的差距，一般越小越好。\n\n\\(L(k,b;T)=\\frac{1}{m}((f(x_1)-y_1)^2+(f(x_2)-y_2)^2+...+(f(x_m)-y_m)^2)=\\frac{1}{m}\\sum\\limits_{i=1}^m(f(x_i)-y_i)^2\\)\n\n学习问题（最优化问题）：选择某种算法，如导数法、配方法、梯度下降法等，求解使得 \\(L(k,b;T)\\) 取最小值时的参数 \\(k,b\\)"
  },
  {
    "objectID": "Lec1.html#建模-1",
    "href": "Lec1.html#建模-1",
    "title": "0x01 线性回归",
    "section": "建模",
    "text": "建模"
  },
  {
    "objectID": "Lec1.html#练习",
    "href": "Lec1.html#练习",
    "title": "0x01 线性回归",
    "section": "练习",
    "text": "练习\n\n给定训练集 \\(T=\\{(1,2),(2,3),(3,4),(4,2)\\}\\)， \\(m=4\\)\n决策函数：假设 \\(f(x)=kx\\)\n写出损失函数，并求最优参数 \\(k\\)"
  },
  {
    "objectID": "Lec1.html#切线与导数",
    "href": "Lec1.html#切线与导数",
    "title": "0x01 线性回归",
    "section": "切线与导数",
    "text": "切线与导数\n\n割线：记 \\(A\\) 为图像上一个定点，\\(B\\) 为图像上 \\(A\\) 附近的点，称直线 \\(AB\\) 为图像的割线\n切线：当 \\(B\\) 无限接近 \\(A\\) 时，割线 \\(AB\\) 无限接近经过 \\(A\\) 的一条直线 \\(l\\)，称 \\(l\\) 为图在点 \\(A\\) 处的切线。切线反映了切点附近函数值的变化趋势\n导数：切线的斜率"
  },
  {
    "objectID": "Lec1.html#例fxfrac18x2-在点-a20.5-处的切线",
    "href": "Lec1.html#例fxfrac18x2-在点-a20.5-处的切线",
    "title": "0x01 线性回归",
    "section": "例：\\(f(x)=\\frac{1}{8}x^2\\) 在点 \\(A(2,0.5)\\) 处的切线",
    "text": "例：\\(f(x)=\\frac{1}{8}x^2\\) 在点 \\(A(2,0.5)\\) 处的切线"
  },
  {
    "objectID": "Lec1.html#例fxfrac18x2-在点-a20.5-处的切线斜率",
    "href": "Lec1.html#例fxfrac18x2-在点-a20.5-处的切线斜率",
    "title": "0x01 线性回归",
    "section": "例：\\(f(x)=\\frac{1}{8}x^2\\) 在点 \\(A(2,0.5)\\) 处的切线斜率",
    "text": "例：\\(f(x)=\\frac{1}{8}x^2\\) 在点 \\(A(2,0.5)\\) 处的切线斜率\n\n方法一：记 \\(B\\) 点坐标为 \\(B(2+\\Delta x,f(2+\\Delta x))\\)，然后计算直线 \\(AB\\) 的斜率：\\(\\frac{f(2+\\Delta x)-0.5}{2+\\Delta x-2}\\)，给 \\(\\Delta x\\) 一个非常小的值，如 \\(0.00001\\)，得到切线斜率 \\(f'(2)\\) 的近似值，如 \\(0.50000125\\)，如果觉得不满意，就把 \\(\\Delta x\\) 再小一些……\n\ndef f(x):\n    return 1/8*x*x\nprint((f(2+0.00001)-0.5)/0.00001)\n\n方法二：对 \\(\\frac{f(2+\\Delta x)-0.5}{2+\\Delta x-2}\\) 继续进行推导，\\(\\frac{f(2+\\Delta x)-0.5}{2+\\Delta x-2}=\\frac{1}{2}+\\frac{1}{8}\\Delta x\\) ，当 \\(\\Delta x\\rightarrow 0\\) 时，\\(\\frac{f(2+\\Delta x)-0.5}{2+\\Delta x-2}\\rightarrow \\frac{1}{2}\\)，可以认为 \\(f'(2)=\\frac{1}{2}\\)\n事实上，\\(f(x)=\\frac{1}{8}x^2\\) 的过任意点 \\(A(x,f(x))\\) 的切线斜率 \\(f'(x)\\) 都可以按如上方法推导\n事实上，\\(f(x)=ax^2+bx+c\\) 的过点 \\(A(x,f(x))\\) 的切线斜率 \\(f'(x)\\) 也可按如上方法推导"
  },
  {
    "objectID": "Lec1.html#练习-1",
    "href": "Lec1.html#练习-1",
    "title": "0x01 线性回归",
    "section": "练习",
    "text": "练习\n\n推导 \\(f(x)=2x^2+3x-4\\) 在任意点 \\(A(3,f(3))\\) 的导数\n推导 \\(f(x)=ax^2+bx+c\\) 在任意点 \\(A(x,f(x))\\) 的导数，其中 \\(a,b,c\\) 为参数"
  },
  {
    "objectID": "Lec1.html#切线斜率与最值的关系",
    "href": "Lec1.html#切线斜率与最值的关系",
    "title": "0x01 线性回归",
    "section": "切线斜率与最值的关系",
    "text": "切线斜率与最值的关系"
  },
  {
    "objectID": "Lec1.html#梯度下降法",
    "href": "Lec1.html#梯度下降法",
    "title": "0x01 线性回归",
    "section": "梯度下降法",
    "text": "梯度下降法\n\n解析法（配方法）和导数法有局限性，有时损失函数 \\(L\\) 不容易直接求得最值点，或者 \\(L'\\) 不容易求得零点\n梯度下降法：从某一个选定的点出发，沿着梯度（导数）方向移动点，不断接近极值点的数值算法（有误差，但可控）。它是机器学习甚至更广泛领域常见的优化方法。\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPoints\nx\ny\n\n\n\n\nP1\n-2\n65.5\n\n\nP2\n-0.24\n11.292\n\n\nP3\n0.464\n2.61872\n\n\nP4\n0.7456\n1.231\n\n\nP5\n0.85824\n1.00896\n\n\nP6\n0.903296\n0.973433\n\n\nP7\n0.921318\n0.967749"
  },
  {
    "objectID": "Lec1.html#梯度下降法-1",
    "href": "Lec1.html#梯度下降法-1",
    "title": "0x01 线性回归",
    "section": "梯度下降法",
    "text": "梯度下降法\n\n考虑从一个点 \\(x_0\\) 开始逐步向最值点移动，最终能够到达（或无限接近）最值点\n\n选哪个点作为起点 \\(x_0\\) ？“几乎”是任意点！\n每次步长如何确定？\\[ x_{k+1}=x_k-\\alpha f'(x_k) \\]\n算法为什么是正确的？\n\n秘诀：切线指方向，导数定步长"
  },
  {
    "objectID": "Lec1.html#梯度下降法收敛性证明",
    "href": "Lec1.html#梯度下降法收敛性证明",
    "title": "0x01 线性回归",
    "section": "梯度下降法收敛性证明",
    "text": "梯度下降法收敛性证明\n以 \\(f(x)=x^2\\) 为例，任选 \\(x_0\\) 为起点，更新规则 \\(x_{k+1}=x_k-\\alpha f'(x_k)(0\\leqslant \\alpha&lt;\\frac 1 2)\\)，随着\\(k\\)的增大，\\(f(x_k)\\rightarrow f(x^*)\\)，其中 \\(x^*\\) 为 \\(f(x)\\) 取得最小值的点\n\n由梯度下降更新公式：\\(x_{k+1}=x_k-\\alpha f'(x_k) =x_k-2\\alpha  x_k=x_k (1-2 \\alpha )\\)\n\\(x_k =x_{k-1}(1-2\\alpha)=x_{k-2}(1-2\\alpha)^2=x_{k-3}(1-2\\alpha)^3=...=x_{0}(1-2\\alpha)^k\\)\n由 \\(f(x)\\)的凸性，\\(\\forall a,b\\)，\\(f(b)\\geq f(a)+f'(a)(b-a)\\)（可将 \\(f(x)=x^2\\) 代入验证），取 \\(b=x^{*},a=x_k\\)，则有 \\[\\begin{align}\nf(x_k)- f(x^{*})&\\leqslant f'(x_k)(x_k-x^{*})\\\\\n&=2x_k(x_k-x^*)\\\\\n&=2\\left((x_k-\\frac{1}{2} x^*)^2-\\frac{1}{4} (x^*)^2\\right)\n\\end{align}\n\\]\n随着 \\(k\\) 的增大，由于 \\(0\\leqslant (1-2\\alpha)&lt;1\\)，所以当 \\(x_k\\rightarrow 0\\) 时，\\(2\\left((x_k-\\frac{1}{2} x^*)^2-\\frac{1}{4} (x^*)^2\\right)\\rightarrow 0\\)，而 \\(f(x_k)-f(x^*) \\geqslant 0\\)，所以 \\(f(x_k)\\rightarrow f(x^*)\\)"
  },
  {
    "objectID": "Lec1.html#练习-2",
    "href": "Lec1.html#练习-2",
    "title": "0x01 线性回归",
    "section": "练习",
    "text": "练习\n使用梯度下降方法计算 \\(f(x)=2x^2+3x-4\\) 的最值，任选 \\(\\alpha\\) 和 \\(x_0\\)，并与配方法和导数法得到的结果进行比较"
  },
  {
    "objectID": "Lec1.html#思考题",
    "href": "Lec1.html#思考题",
    "title": "0x01 线性回归",
    "section": "思考题",
    "text": "思考题\n以 \\(f(x)=ax^2+bx+c(a&gt;0)\\) 为例，任选 \\(x_0\\) 为起点，更新规则 \\(x_{k+1}=x_k-\\alpha f'(x_k)(0\\leqslant \\alpha&lt;\\frac{1}{2a})\\)，请证明：随着 \\(k\\) 的增大，\\(f(x_k)\\rightarrow f(x^*)\\)，其中 \\(x^*\\) 为 \\(f(x)\\) 取得最小值的点"
  },
  {
    "objectID": "Lec1.html#动手计算",
    "href": "Lec1.html#动手计算",
    "title": "0x01 线性回归",
    "section": "动手计算",
    "text": "动手计算\n\n损失函数 \\(L(k;T)\\) 推导： \\[\\begin{align}\nL(k;T)&=\\frac{1}{m}((f(x_1)-y_1)^2+(f(x_2)-y_2)^2+...+(f(x_m)-y_m)^2)\\\\&=\\frac{1}{m}\\sum\\limits_{i=1}^m(f(x_i)-y_i)^2=\\frac{1}{m}\\sum\\limits_{i=1}^m(kx_i-y_i)^2\\\\&=\\frac{1}{m}\\sum\\limits_{i=1}^m(k^2x_i^2-2kx_iy_i+y_i^2)\\\\&=\\frac{1}{m}((\\sum\\limits_{i=1}^mx_i^2)k^2+(-2\\sum\\limits_{i=1}^mx_iy_i)k+\\sum\\limits_{i=1}^my_i^2)\n\\end{align}\n\\]"
  },
  {
    "objectID": "Lec1.html#动手计算-1",
    "href": "Lec1.html#动手计算-1",
    "title": "0x01 线性回归",
    "section": "动手计算",
    "text": "动手计算\n\n\\(L(k;T)=\\frac{1}{m}((\\sum\\limits_{i=1}^mx_i^2)k^2+(-2\\sum\\limits_{i=1}^mx_iy_i)k+\\sum\\limits_{i=1}^my_i^2)\\)\n\\(L'(k;T)=\\frac{1}{m}(2(\\sum\\limits_{i=1}^mx_i^2)k-2\\sum\\limits_{i=1}^mx_iy_i)\\)，\\(k^*=\\frac{\\sum_{i=1}^mx_iy_i}{\\sum_{i=1}^mx_i^2}\\)\n方法一：手动计算（只要有足够的时间和耐心）\n方法二：Excel\n\n借助公式辅助手动计算（略）\n调用公式；使用散点图\n\n方法三：python\n\n编程模拟计算过程：解析法和导数法；梯度下降法\n调用现成的工具（如sklearn）"
  },
  {
    "objectID": "Lec1.html#动手计算excel直接调用公式法",
    "href": "Lec1.html#动手计算excel直接调用公式法",
    "title": "0x01 线性回归",
    "section": "动手计算——Excel直接调用公式法",
    "text": "动手计算——Excel直接调用公式法\n\n在数据旁边选择一个空单元格，输入=LINEST(B2:B28,A2:A28,FALSE)\n\n=表示此单元格内容为公式\nLINEST表示求解线性回归参数的函数\nB2:B28处是 \\(y\\) 值的范围\nA2:A28处是 \\(x\\) 值的范围\nFALSE表示强制截距 \\(b\\) 为 \\(0\\)"
  },
  {
    "objectID": "Lec1.html#动手计算excel散点图添加趋势线法",
    "href": "Lec1.html#动手计算excel散点图添加趋势线法",
    "title": "0x01 线性回归",
    "section": "动手计算——Excel散点图添加趋势线法",
    "text": "动手计算——Excel散点图添加趋势线法\n\n选中所有数据（包括列标题）\n点击“插入”——“散点图”\n点击图中任意一点（所有点将被选中）\n右键选择“添加趋势线”\n在右侧弹出的“设置趋势线格式”中勾选“设置截距”和“显示公式”"
  },
  {
    "objectID": "Lec1.html#python实现梯度下降",
    "href": "Lec1.html#python实现梯度下降",
    "title": "0x01 线性回归",
    "section": "python实现梯度下降",
    "text": "python实现梯度下降\n\n以计算 \\(f(x)=\\frac{1}{4}(30k^2-56k+30)\\) 的最小值为例\n代码中#及之后的内容是注释，不会被运行\na=b是一个动作，可以不严谨地理解为把b代表的内容“传递”给a，即 \\(a\\leftarrow b\\)\n\ndef f(x): # 定义f(x)的计算方法\n    return (30*x*x-56*x+30)/4\n\ndef ff(x): # 定义f'(x)的计算方法\n    return (60*x-56)/4\n\nx=-2 # x0=-2\nalpha=0.04\nfor i in range(1,10+1):\n    # 将下一行的注释取消，可以输出每一步的计算过程\n    # print(f\"x{i-1}={x:.3f}, f'(x{i-1})=2*{x:.3f}+4={ff(x):.3f}; x{i}=x{i-1}-alpha*f'(x{i-1})={x:.3f}-{alpha}*{ff(x):.3f}={x-alpha*ff(x):.3f}\")\n    x=x-alpha*ff(x) # 计算下一个点\n\nprint(x) # 输出最后的x\nprint(f(x)) # 输出最后的f(x)"
  },
  {
    "objectID": "Lec1.html#python实现线性回归",
    "href": "Lec1.html#python实现线性回归",
    "title": "0x01 线性回归",
    "section": "python实现线性回归",
    "text": "python实现线性回归\n\n\nfrom sklearn.linear_model import LinearRegression\nimport pandas as pd\nimport matplotlib.pyplot as plt\ndf=pd.read_csv(\"house.csv\")  # house.csv是数据源\nx=df[[\"area\"]] # area指明x列标题\ny=df[\"price\"] # price指明y列标题\nmodel=LinearRegression(fit_intercept=False) # 调用线性回归模型，且设置截距b=0\nmodel.fit(x,y) # 用把数据传递模型\nk=model.coef_ # 获取斜率结果\nprint(\"斜率\",model.coef_[0])\n\nimport numpy as np\ndef f(x):\n    return k*x\nfig=plt.figure(figsize=(15,9))\nax = fig.add_subplot(111)\nx=np.arange(min(x[\"area\"]),max(x[\"area\"]),0.01)\ny=f(x)\nax.plot(x,y)\nx=list(df[\"area\"])\ny=list(df[\"price\"])\nax.scatter(x,y)\nplt.show()\n\n\n\n斜率 10.114737008812858"
  },
  {
    "objectID": "Lec1.html#求导示例",
    "href": "Lec1.html#求导示例",
    "title": "0x01 线性回归",
    "section": "求导示例",
    "text": "求导示例\n\n\\(f(x)=\\frac{1}{8}x^2\\) 在点 \\(A(2,0.5)\\) 处的切线斜率（导数）\n\n记 \\(A\\) 点附近的 \\(B\\) 点坐标为 \\((2+\\Delta x,f(2+\\Delta x))\\)\n割线 \\(AB\\) 的斜率为 \\[\n\\frac{f(2+\\Delta x)-f(2)}{2+\\Delta x-2}=\\frac{\\frac{1}{8}(2+\\Delta x)^2-\\frac{1}{8}2^2}{\\Delta x}=\\frac{1}{8}\\frac{4+4\\Delta x+\\Delta x^2-4}{\\Delta x}=\\frac{1}{2}+\\frac{1}{8}\\Delta x\n\\]\n当 \\(\\Delta x\\rightarrow 0\\) 时，割线的斜率越近于 \\(\\frac{1}{2}\\)，故 \\(f'(2)=\\frac{1}{2}\\)\n\n\\(f(x)=\\frac{1}{8}x^2\\) 在任意点 \\(A(x,f(x))\\) 处的切线斜率（导数）\n\n记 \\(A\\) 点附近的 \\(B\\) 点坐标为 \\((x+\\Delta x,f(x+\\Delta x))\\)\n割线 \\(AB\\) 的斜率为 \\[\n\\frac{f(2+\\Delta x)-f(2)}{2+\\Delta x-2}=\\frac{\\frac{1}{8}(2+\\Delta x)^2-\\frac{1}{8}2^2}{\\Delta x}=\\frac{1}{8}\\frac{4+4\\Delta x+\\Delta x^2-4}{\\Delta x}=\\frac{1}{2}+\\frac{1}{8}\\Delta x\n\\]\n当 \\(\\Delta x\\rightarrow 0\\) 时，割线的斜率越近于 \\(\\frac{1}{2}\\)，故 \\(f'(2)=\\frac{1}{2}\\)"
  },
  {
    "objectID": "Lec1.html#求导示例fxfrac18x2-在点-a20.5-处的切线斜率导数",
    "href": "Lec1.html#求导示例fxfrac18x2-在点-a20.5-处的切线斜率导数",
    "title": "0x01 线性回归",
    "section": "求导示例：\\(f(x)=\\frac{1}{8}x^2\\) 在点 \\(A(2,0.5)\\) 处的切线斜率（导数）",
    "text": "求导示例：\\(f(x)=\\frac{1}{8}x^2\\) 在点 \\(A(2,0.5)\\) 处的切线斜率（导数）\n- 记 $A$ 点附近的 $B$ 点坐标为 $(2+\\Delta x,f(2+\\Delta x))$\n- 割线 $AB$ 的斜率为\n\\[\n\\frac{f(2+\\Delta x)-f(2)}{2+\\Delta x-2}=\\frac{\\frac{1}{8}(2+\\Delta x)^2-\\frac{1}{8}2^2}{\\Delta x}=\\frac{1}{8}\\frac{4+4\\Delta x+\\Delta x^2-4}{\\Delta x}=\\frac{1}{2}+\\frac{1}{8}\\Delta x\n\\]\n当 \\(\\Delta x\\rightarrow 0\\) 时，割线的斜率越近于 \\(\\frac{1}{2}\\)，故 \\(f'(2)=\\frac{1}{2}\\)"
  },
  {
    "objectID": "Lec1.html#求导示例fxfrac18x2-在任意点-axfx-处的切线斜率导数",
    "href": "Lec1.html#求导示例fxfrac18x2-在任意点-axfx-处的切线斜率导数",
    "title": "0x01 线性回归",
    "section": "求导示例：\\(f(x)=\\frac{1}{8}x^2\\) 在任意点 \\(A(x,f(x))\\) 处的切线斜率（导数）",
    "text": "求导示例：\\(f(x)=\\frac{1}{8}x^2\\) 在任意点 \\(A(x,f(x))\\) 处的切线斜率（导数）\n- 记 $A$ 点附近的 $B$ 点坐标为 $(x+\\Delta x,f(x+\\Delta x))$\n- 割线 $AB$ 的斜率为\n\\[\n\\frac{f(x+\\Delta x)-f(x)}{x+\\Delta x-x}=\\frac{\\frac{1}{8}(x+\\Delta x)^2-\\frac{1}{8}x^2}{\\Delta x}=\\frac{1}{8}\\frac{x+2x\\Delta x+\\Delta x^2-x^2}{\\Delta x}=\\frac{1}{2}+\\frac{1}{8}\\Delta x\n\\]\n\\(\\Delta x\\rightarrow 0\\) 时，割线的斜率越近于 \\(\\frac{1}{2}\\)，故 \\(f'(2)=\\frac{1}{2}\\)"
  },
  {
    "objectID": "Lec1.html#示例fxfrac18x2-在点-a20.5-处的切线斜率导数",
    "href": "Lec1.html#示例fxfrac18x2-在点-a20.5-处的切线斜率导数",
    "title": "0x01 线性回归",
    "section": "示例：\\(f(x)=\\frac{1}{8}x^2\\) 在点 \\(A(2,0.5)\\) 处的切线斜率（导数）",
    "text": "示例：\\(f(x)=\\frac{1}{8}x^2\\) 在点 \\(A(2,0.5)\\) 处的切线斜率（导数）\n- 记 $A$ 点附近的 $B$ 点坐标为 $(2+\\Delta x,f(2+\\Delta x))$\n- 割线 $AB$ 的斜率为\n\\[\n\\frac{f(2+\\Delta x)-f(2)}{2+\\Delta x-2}=\\frac{\\frac{1}{8}(2+\\Delta x)^2-\\frac{1}{8}2^2}{\\Delta x}=\\frac{1}{8}\\frac{4+4\\Delta x+\\Delta x^2-4}{\\Delta x}=\\frac{1}{2}+\\frac{1}{8}\\Delta x\n\\]\n当 \\(\\Delta x\\rightarrow 0\\) 时，割线的斜率越近于 \\(\\frac{1}{2}\\)，故 \\(f'(2)=\\frac{1}{2}\\)"
  },
  {
    "objectID": "Lec1.html#示例fxfrac18x2-在任意点-axfx-处的切线斜率导数",
    "href": "Lec1.html#示例fxfrac18x2-在任意点-axfx-处的切线斜率导数",
    "title": "0x01 线性回归",
    "section": "示例：\\(f(x)=\\frac{1}{8}x^2\\) 在任意点 \\(A(x,f(x))\\) 处的切线斜率（导数）",
    "text": "示例：\\(f(x)=\\frac{1}{8}x^2\\) 在任意点 \\(A(x,f(x))\\) 处的切线斜率（导数）\n- 记 $A$ 点附近的 $B$ 点坐标为 $(x+\\Delta x,f(x+\\Delta x))$\n- 割线 $AB$ 的斜率为\n\\[\n\\frac{f(x+\\Delta x)-f(x)}{x+\\Delta x-x}=\\frac{\\frac{1}{8}(x+\\Delta x)^2-\\frac{1}{8}x^2}{\\Delta x}=\\frac{1}{8}\\frac{x+2x\\Delta x+\\Delta x^2-x^2}{\\Delta x}=\\frac{1}{2}+\\frac{1}{8}\\Delta x\n\\]\n\\(\\Delta x\\rightarrow 0\\) 时，割线的斜率越近于 \\(\\frac{1}{2}\\)，故 \\(f'(2)=\\frac{1}{2}\\)"
  },
  {
    "objectID": "Lec1.html#示例fxfrac18x2-在点-a20.5-处的切线斜率",
    "href": "Lec1.html#示例fxfrac18x2-在点-a20.5-处的切线斜率",
    "title": "0x01 线性回归",
    "section": "示例：\\(f(x)=\\frac{1}{8}x^2\\) 在点 \\(A(2,0.5)\\) 处的切线斜率",
    "text": "示例：\\(f(x)=\\frac{1}{8}x^2\\) 在点 \\(A(2,0.5)\\) 处的切线斜率\n\n记 \\(A\\) 点附近的 \\(B\\) 点坐标为 \\((2+\\Delta x,f(2+\\Delta x))\\)\n割线 \\(AB\\) 的斜率为 \\[\n\\frac{f(2+\\Delta x)-f(2)}{2+\\Delta x-2}=\\frac{\\frac{1}{8}(2+\\Delta x)^2-\\frac{1}{8}2^2}{\\Delta x}=\\frac{1}{8}\\frac{4+4\\Delta x+\\Delta x^2-4}{\\Delta x}=\\frac{1}{2}+\\frac{1}{8}\\Delta x\n\\]\n\n当 \\(\\Delta x\\rightarrow 0\\) 时，割线的斜率越近于 \\(\\frac{1}{2}\\)，故 \\(f'(2)=\\frac{1}{2}\\)"
  },
  {
    "objectID": "Lec1.html#示例fxfrac18x2-在任意点-axfx-处的切线斜率",
    "href": "Lec1.html#示例fxfrac18x2-在任意点-axfx-处的切线斜率",
    "title": "0x01 线性回归",
    "section": "示例：\\(f(x)=\\frac{1}{8}x^2\\) 在任意点 \\(A(x,f(x))\\) 处的切线斜率",
    "text": "示例：\\(f(x)=\\frac{1}{8}x^2\\) 在任意点 \\(A(x,f(x))\\) 处的切线斜率\n\n记 \\(A\\) 点附近的 \\(B\\) 点坐标为 \\((x+\\Delta x,f(x+\\Delta x))\\)\n割线 \\(AB\\) 的斜率为 \\[\n\\frac{f(x+\\Delta x)-f(x)}{x+\\Delta x-x}=\\frac{\\frac{1}{8}(x+\\Delta x)^2-\\frac{1}{8}x^2}{\\Delta x}=\\frac{1}{8}\\frac{x^2+2x\\Delta x+\\Delta x^2-x^2}{\\Delta x}=\\frac{1}{4}x+\\frac{1}{8}\\Delta x\n\\]\n\n\\(\\Delta x\\rightarrow 0\\) 时，割线的斜率越近于 \\(\\frac{1}{4}x\\)，故 \\(f'(x)=\\frac{1}{4}x\\)"
  },
  {
    "objectID": "Lec1.html#配方法",
    "href": "Lec1.html#配方法",
    "title": "0x01 线性回归",
    "section": "配方法",
    "text": "配方法\n\\[\\begin{align}\nf(x,y)&=\\frac{1}{4}(30x^2+4y^2+20xy-56x-20y+30)\\\\&=\\frac{1}{4}((5x+2y-5)^2+5x^2-6x+5)\n\\end{align}\n\\]\n\n当 \\(x=\\frac{6}{10}\\) 时，\\(5x^2-6x+5\\) 取得最小值，\\(y=1\\) 时，\\((5x+2y-5)^2\\) 可取到 \\(0\\)\n因此最小值点为 \\((\\frac{3}{5},1)\\)，最小值为 \\(\\frac{4}{5}\\)"
  },
  {
    "objectID": "Lec1.html#练习-3",
    "href": "Lec1.html#练习-3",
    "title": "0x01 线性回归",
    "section": "练习",
    "text": "练习\n\n用配方法求 \\(f(x,y)=\\frac{1}{2} x^{2}+\\frac{1}{4} y^{2}+\\frac{1}{4}x y-2 x-y+3\\) 的最小值"
  },
  {
    "objectID": "Lec1.html#导数法",
    "href": "Lec1.html#导数法",
    "title": "0x01 线性回归",
    "section": "导数法",
    "text": "导数法\n认识 \\(f(x,y)=\\frac{1}{4}(30x^2+4y^2+20xy-56x-20y+30)\\)图像"
  },
  {
    "objectID": "Lec1.html#导数法-1",
    "href": "Lec1.html#导数法-1",
    "title": "0x01 线性回归",
    "section": "导数法",
    "text": "导数法"
  },
  {
    "objectID": "Lec1.html#导数法-2",
    "href": "Lec1.html#导数法-2",
    "title": "0x01 线性回归",
    "section": "导数法",
    "text": "导数法\n\n在已知 \\(y\\) 的情况下，\\(f(x,y)\\) 可以看作以 \\(x\\) 为自变量的一元二次函数 \\(f(x;y)\\)，因此 \\(f(x;y)\\) 有关于 \\(x\\) 的导数（或在 \\(x\\) 方向的切线），记为 \\(f'(x;y)\\)（或 \\(f_x(x,y)\\)、\\(\\frac{\\partial f}{\\partial x}\\)）\n在已知 \\(x\\) 的情况下，类似地，有\\(f'(y;x)\\)（或 \\(f_y(x,y)\\)、\\(\\frac{\\partial f}{\\partial y}\\)）\n对于 \\(f(x,y)=\\frac{1}{4}(30 x^2+ 4 y^2+ 20xy- 56x - 20y  +30   )\\)\n\n\\(f_x(x,y)=\\frac{1}{4}(60x+20y-56)\\)\n\\(f_y(x,y)=\\frac{1}{4}(8y+20x-20)\\)\n令 \\(f_x(x,y)=0\\)且\\(f_y(x,y)=0\\)，得到 \\(x=\\frac{3}{5},y=1\\)"
  },
  {
    "objectID": "Lec1.html#练习-4",
    "href": "Lec1.html#练习-4",
    "title": "0x01 线性回归",
    "section": "练习",
    "text": "练习\n用导数法求 \\(f(x,y)=\\frac{1}{2} x^{2}+\\frac{1}{4} y^{2}+\\frac{1}{4}x y-2 x-y+3\\) 的最小值"
  },
  {
    "objectID": "Lec1.html#梯度下降法-2",
    "href": "Lec1.html#梯度下降法-2",
    "title": "0x01 线性回归",
    "section": "梯度下降法",
    "text": "梯度下降法\n\n\n\n\n\n\n\n\n\n\n\n\n\n口诀：切线指方向，导数定步长\n任选 \\((x_0,y_0)\\) ，每次同时更新： \\[\n\\begin{cases}\nx_{k+1}=x_k-\\alpha f_x(x_k,y_k)\\\\\ny_{k+1}=y_k-\\alpha f_y(x_k,y_k)\n\\end{cases}\n\\]\n可以证明，双参数线性回归损失函数在选取合适的 \\(\\alpha\\) 时，可以收敛到最小值\n更一般地，二元二次函数\\(f(x,y)\\)满足一定的条件（凸性、可微与Lipschitz连续性）时，梯度下降可以收敛最小值"
  },
  {
    "objectID": "Lec1.html#动手计算-2",
    "href": "Lec1.html#动手计算-2",
    "title": "0x01 线性回归",
    "section": "动手计算",
    "text": "动手计算\n\n\\(L(k,b;T) =  (\\sum_{i=1}^{m} x_i^2)k^2 + mb^2+ 2( \\sum_{i=1}^{m} x_i)k b - 2 (\\sum_{i=1}^{m}x_i y_i)k- 2 (\\sum_{i=1}^{m} y_i)b  +\\sum_{i=1}^{m} y_i^2\\)\n记 \\(A=\\sum x_i^2,B=\\sum x_i,C=\\sum y_i,D=\\sum x_iy_i,E=\\sum y_i^2,\\bar{x}=\\frac{B}{m}=\\frac{\\sum x_i}{m},\\bar{y}=\\frac{C}{m}=\\frac{\\sum y_i}{m}\\)\n\\(L(k,b;T) =Ak^2+mb^2+2Bkb-2Dk-2Cb+E\\)，先对 \\(b\\) 项配方，再对 \\(k\\) 项配方，得\n\n\\(L(k,b;T)=m(b+\\frac{Bk-C}{m})^2+(\\frac{mA-B^2}{m})(k+\\frac{BC-Dm}{mA-B^2})^2+\\text{常数项}\\)\n\\(k^*=\\frac{Dm-BC}{mA-B^2}=\\frac{\\sum x_iy_i-m\\bar{x}\\bar{y}}{\\sum x_i^2-m\\bar{x}^2},b^*=\\bar{y}-\\bar{x}k\\)\n\n\\(L_k(k,b;T)=2Ak+2Bb-2D=2(Ak+Bb-D)\\)\n\\(L_b(k,b;T)=2mb+2Bk-2C=2(Bk+mb-C)\\)"
  },
  {
    "objectID": "Lec1.html#动手计算-3",
    "href": "Lec1.html#动手计算-3",
    "title": "0x01 线性回归",
    "section": "动手计算",
    "text": "动手计算\n\n方法一：手动计算（只要有足够的时间和耐心）\n方法二：Excel\n\n借助公式辅助手动计算（略）\n调用公式（FALSE改为TRUE）；使用散点图（不设置截距）\n\n方法三：python\n\n编程模拟计算过程：解析法和导数法（略）；梯度下降法\n调用现成的工具（如sklearn，fit_intercept=True）"
  },
  {
    "objectID": "Lec1.html#python实现梯度下降法",
    "href": "Lec1.html#python实现梯度下降法",
    "title": "0x01 线性回归",
    "section": "python实现梯度下降法",
    "text": "python实现梯度下降法\n以计算 \\(f(x,y)=\\frac{1}{4}(30x^2+4y^2+20xy-56x-20y+30)\\) 最小值为例\ndef f(x,y):\n    return (30*x**2 +4* y**2+20*x*y-56*x-20*y+30)/4\ndef g(x,y): # f在x方向的偏导数\n    return (60*x+20*y-56)/4\ndef h(x,y): # f在y方向的偏导数\n    return (8*y+20*x-20)/4\n\nalpha=0.005 # 学习率\nx,y=1,4\nfor i in range(1,cnt+1):\n    # 将下一行的注释取消，可以输出每一步的计算过程\n    # print(f\"x{i-1}={x:.3f},y{i-1}={y:.3f} ; x{i}={x-alpha*g(x,y):.3f},y{i}={y-alpha*h(x,y):.3f},f(x{i},y{i})={f(x-alpha*g(x,y),y-alpha*h(x,y)):.3f}\")\n    x_,y_=x-alpha*g(x,y),y-alpha*h(x,y)\nprint(\"the best x,y and f(x,y):\",x,y,f(x,y)) # 输出最后的x,f(x)"
  },
  {
    "objectID": "Lec1.html#python实现线性回归解析法或导数法",
    "href": "Lec1.html#python实现线性回归解析法或导数法",
    "title": "0x01 线性回归",
    "section": "python实现线性回归——解析法或导数法",
    "text": "python实现线性回归——解析法或导数法\n\n本质是整理损失函数的系数，再使用 \\(k^*=-\\frac{b}{2a}\\)\n此处略去python实现"
  },
  {
    "objectID": "Lec1.html#python实现线性回归调用现成的sklearn库",
    "href": "Lec1.html#python实现线性回归调用现成的sklearn库",
    "title": "0x01 线性回归",
    "section": "python实现线性回归——调用现成的sklearn库",
    "text": "python实现线性回归——调用现成的sklearn库\n\n\nfrom sklearn.linear_model import LinearRegression\nimport pandas as pd\nimport matplotlib.pyplot as plt\ndf=pd.read_csv(\"house.csv\")  # house.csv是数据源\nx=df[[\"area\"]] # area指明x列标题\ny=df[\"price\"] # price指明y列标题\nmodel=LinearRegression(fit_intercept=False) # 调用线性回归模型，且设置截距b=0\nmodel.fit(x,y) # 用把数据传递模型\nk=model.coef_ # 获取斜率结果\nprint(\"斜率\",model.coef_[0])\n\nimport numpy as np\ndef f(x):\n    return k*x\nfig=plt.figure(figsize=(15,9))\nax = fig.add_subplot(111)\nx=np.arange(min(x[\"area\"]),max(x[\"area\"]),0.01)\ny=f(x)\nax.plot(x,y)\nx=list(df[\"area\"])\ny=list(df[\"price\"])\nax.scatter(x,y)\nplt.show()\n\n\n\n斜率 10.114737008812858"
  },
  {
    "objectID": "Lec1.html#python实现线性回归调用现成的sklearn库-1",
    "href": "Lec1.html#python实现线性回归调用现成的sklearn库-1",
    "title": "0x01 线性回归",
    "section": "python实现线性回归——调用现成的sklearn库",
    "text": "python实现线性回归——调用现成的sklearn库\n\n\nfrom sklearn.linear_model import LinearRegression\nimport pandas as pd\nimport matplotlib.pyplot as plt\ndf=pd.read_csv(\"house.csv\")  # house.csv是数据源\nx=df[[\"area\"]] # area指明x列标题\ny=df[\"price\"] # price指明y列标题\nmodel=LinearRegression(fit_intercept=True) # 调用线性回归模型\nmodel.fit(x,y) # 用把数据传递模型\nprint(\"斜率\",model.coef_[0])\nprint(\"截距\",model.intercept_)\n\nimport numpy as np\ndef f(x):\n    return k*x\nfig=plt.figure(figsize=(15,9))\nax = fig.add_subplot(111)\nx=np.arange(min(x[\"area\"]),max(x[\"area\"]),0.01)\ny=f(x)\nax.plot(x,y)\nx=list(df[\"area\"])\ny=list(df[\"price\"])\nax.scatter(x,y)\nplt.show()\n\n\n\n斜率 8.886379927886498\n截距 73.98296736905706"
  },
  {
    "objectID": "Lec1.html#梯度下降法计算实例",
    "href": "Lec1.html#梯度下降法计算实例",
    "title": "0x01 线性回归",
    "section": "梯度下降法——计算实例",
    "text": "梯度下降法——计算实例\n\\(f(x)=x^2+4x-1\\)，\\(f'(x)=2x+4\\)，\\(\\alpha=0.4\\)，\\(x_0=5\\) \\[\nx_0=\\ 5.000, f'(x_0)=14.000; x_1=x_0-\\alpha f'(x_0)=5.000-0.4\\times 14.000=-0.600\\\\\nx_1=-0.600, f'(x_1)=2.800; x_2=x_1-\\alpha f'(x_1)=-0.600-0.4\\times 2.800=-1.720\\\\\nx_2=-1.720, f'(x_2)=0.560; x_3=x_2-\\alpha f'(x_2)=-1.720-0.4\\times 0.560=-1.944\\\\\nx_3=-1.944, f'(x_3)=0.112; x_4=x_3-\\alpha f'(x_3)=-1.944-0.4\\times 0.112=-1.989\\\\\nx_4=-1.989, f'(x_4)=0.022; x_5=x_4-\\alpha f'(x_4)=-1.989-0.4\\times 0.022=-1.998\\\\\nx_5=-1.998, f'(x_5)=0.004; x_6=x_5-\\alpha f'(x_5)=-1.998-0.4\\times 0.004=-2.000\n\\]"
  },
  {
    "objectID": "Lec1.html#盲人摸象",
    "href": "Lec1.html#盲人摸象",
    "title": "0x01 线性回归",
    "section": "“盲人摸象”",
    "text": "“盲人摸象”\n\nhttps://www.geogebra.org/classic/jdjne9bg\n等高线：https://jc.pep.com.cn/\n近似理解：https://www.geogebra.org/classic/xyptd2hw"
  },
  {
    "objectID": "Lec1.html#梯度下降的理解",
    "href": "Lec1.html#梯度下降的理解",
    "title": "0x01 线性回归",
    "section": "梯度下降的理解",
    "text": "梯度下降的理解\n\n“盲人摸象”：https://www.geogebra.org/classic/jdjne9bg\n等高线：https://jc.pep.com.cn/\n近似理解：https://www.geogebra.org/classic/xyptd2hw"
  },
  {
    "objectID": "Lec1.html#盲人下山",
    "href": "Lec1.html#盲人下山",
    "title": "0x01 线性回归",
    "section": "“盲人下山”",
    "text": "“盲人下山”\n\nhttps://www.geogebra.org/classic/jdjne9bg"
  },
  {
    "objectID": "Lec1.html#梯度下降理解",
    "href": "Lec1.html#梯度下降理解",
    "title": "0x01 线性回归",
    "section": "梯度下降理解",
    "text": "梯度下降理解"
  },
  {
    "objectID": "Lec1.html#梯度下降理解-1",
    "href": "Lec1.html#梯度下降理解-1",
    "title": "0x01 线性回归",
    "section": "梯度下降理解",
    "text": "梯度下降理解\n\n数形结合\n\n什么是等高线：https://jc.pep.com.cn/\nhttps://www.geogebra.org/classic/jdjne9bg\n\n近似理解：https://www.geogebra.org/classic/xyptd2hw"
  },
  {
    "objectID": "Lec1.html#下坡时的路线选择",
    "href": "Lec1.html#下坡时的路线选择",
    "title": "0x01 线性回归",
    "section": "下坡时的路线选择",
    "text": "下坡时的路线选择"
  },
  {
    "objectID": "Lec1.html#fxy-在-x_0y_0-点的梯度是-f_xx_0y_0f_yx_0y_0",
    "href": "Lec1.html#fxy-在-x_0y_0-点的梯度是-f_xx_0y_0f_yx_0y_0",
    "title": "0x01 线性回归",
    "section": "\\(f(x,y)\\) 在 \\((x_0,y_0)\\) 点的梯度是 \\(f_x(x_0,y_0):f_y(x_0,y_0)\\)",
    "text": "\\(f(x,y)\\) 在 \\((x_0,y_0)\\) 点的梯度是 \\(f_x(x_0,y_0):f_y(x_0,y_0)\\)\n\n遇事不决先“近似”——斜坡近似：https://www.geogebra.org/classic/xyptd2hw\n下坡时的路线选择"
  },
  {
    "objectID": "Lec1.html#梯度理解",
    "href": "Lec1.html#梯度理解",
    "title": "0x01 线性回归",
    "section": "梯度理解",
    "text": "梯度理解\n\n遇事不决先“近似”\n梯度首先是 \\(xy\\) 平面上的一个方向，什么是方向？\n\n可以看作是一个比例 \\(a:b\\)\n如果要从一个点 \\((x,y)\\) 出发沿着方向 \\(a:b\\) 走，那么只能走到 \\(x+ak,y+bk\\)，\\(k\\in R\\)\n\n梯度是函数值 \\(f(x,y)\\) 增长（趋势）最快的方向（反之，梯度的反方向就是 \\(f(x,y)\\) 下降（趋势）最快的方向）\n\n最快：“走”同样的路程，函数值上升（下降）的最多\n梯度的反方向的理解：下坡时的路线选择"
  },
  {
    "objectID": "Lec1.html#斜坡近似",
    "href": "Lec1.html#斜坡近似",
    "title": "0x01 线性回归",
    "section": "斜坡近似",
    "text": "斜坡近似\n\n什么是等高线：https://jc.pep.com.cn/\nhttps://www.geogebra.org/classic/jdjne9bg\n斜坡近似：https://www.geogebra.org/classic/xyptd2hw"
  },
  {
    "objectID": "Lec1.html#梯度",
    "href": "Lec1.html#梯度",
    "title": "0x01 线性回归",
    "section": "梯度",
    "text": "梯度\n\n梯度首先是 \\(xy\\) 平面上的一个方向，什么是方向？\n\n可以看作是一个比例 \\(a:b\\)（注：在高等数学中并非用此比例符号表示梯度）\n如果要从一个点 \\((x,y)\\) 出发水平方向沿着 \\(a:b\\) 走，那么只能走到 \\((x+ak,y+bk)\\)，\\(k\\in R\\)\n\n每个点都有一个相关的梯度\n梯度是这个点的函数值 \\(f(x,y)\\) 增长（趋势）最快的那个方向（反之，梯度的反方向就是 \\(f(x,y)\\) 下降（趋势）最快的方向）\n\n最快：“走”同样的路程，函数值上升（下降）的最多"
  },
  {
    "objectID": "Lec1.html#梯度-1",
    "href": "Lec1.html#梯度-1",
    "title": "0x01 线性回归",
    "section": "梯度",
    "text": "梯度\n\n什么是等高线：https://jc.pep.com.cn/\n函数梯度示例：https://www.geogebra.org/classic/jdjne9bg"
  },
  {
    "objectID": "Lec1.html#fxy-在-x_0y_0-点的梯度是-f_xx_0y_0f_yx_0y_0-1",
    "href": "Lec1.html#fxy-在-x_0y_0-点的梯度是-f_xx_0y_0f_yx_0y_0-1",
    "title": "0x01 线性回归",
    "section": "\\(f(x,y)\\) 在 \\((x_0,y_0)\\) 点的梯度是 \\(f_x(x_0,y_0):f_y(x_0,y_0)\\)",
    "text": "\\(f(x,y)\\) 在 \\((x_0,y_0)\\) 点的梯度是 \\(f_x(x_0,y_0):f_y(x_0,y_0)\\)\n\n斜坡近似+几何分析：https://www.geogebra.org/classic/xyptd2hw\n已画好的：https://www.geogebra.org/classic/hwv93xnf"
  },
  {
    "objectID": "Lec1.html#梯度下降示例",
    "href": "Lec1.html#梯度下降示例",
    "title": "0x01 线性回归",
    "section": "梯度下降示例",
    "text": "梯度下降示例\n\n图例：https://www.geogebra.org/classic/jdjne9bg\n计算示例：见稍后玻尔平台演示"
  },
  {
    "objectID": "Lec1.html#建模流程",
    "href": "Lec1.html#建模流程",
    "title": "0x01 线性回归",
    "section": "建模流程",
    "text": "建模流程\n\n训练集 \\(T\\)：采集到的样本数据\n\n一个训练数据记为 \\((x,y)\\)，第 \\(i\\) 个训练数据记为 \\((x_i,y_i)\\)\n训练集大小：\\(m\\)\n\n\\(T=\\{(x_1,y_1),(x_2,y_2),...,(x_m,y_m)\\}=\\{(x_i,y_i)\\}\\)\n\n决策函数假设：\\(f(x)=kx+b\\)，属性 \\(x\\) 相应的预测结果 \\(f(x)\\)\n损失函数: \\(L(k,b;T)\\)，在已知训练集 \\(T\\) 的情况下关于决策函数参数 \\(k,b\\) 的函数，用于量化参数的效果，即在训练集上预测值与真实值的差距，一般越小越好。\n\n\\(L(k,b;T)=\\frac{1}{m}((f(x_1)-y_1)^2+(f(x_2)-y_2)^2+...+(f(x_m)-y_m)^2)=\\frac{1}{m}\\sum\\limits_{i=1}^m(f(x_i)-y_i)^2\\)\n\n学习问题（最优化问题）：选择某种算法，如导数法、配方法、梯度下降法等，求解使得 \\(L(k,b;T)\\) 取最小值时的参数 \\(k,b\\)"
  },
  {
    "objectID": "Lec1.html#建模流程-1",
    "href": "Lec1.html#建模流程-1",
    "title": "0x01 线性回归",
    "section": "建模流程",
    "text": "建模流程"
  },
  {
    "objectID": "Lec1.html#求最小值的方法",
    "href": "Lec1.html#求最小值的方法",
    "title": "0x01 线性回归",
    "section": "求最小值的方法",
    "text": "求最小值的方法\n\n配方法\n导数法\n梯度下降法"
  },
  {
    "objectID": "Lec1.html#下节预告",
    "href": "Lec1.html#下节预告",
    "title": "0x01 线性回归",
    "section": "下节预告",
    "text": "下节预告"
  },
  {
    "objectID": "Lec1.html#下节预告感知机",
    "href": "Lec1.html#下节预告感知机",
    "title": "0x01 线性回归",
    "section": "下节预告——感知机",
    "text": "下节预告——感知机\n\n二分类问题\n新的损失函数与梯度下降方法\n新的平面几何知识"
  }
]